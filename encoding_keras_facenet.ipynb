{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ac65f5f",
   "metadata": {},
   "source": [
    "# Section 1. Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df56676e-6e25-4dd8-9c8d-55dc9e54a8d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.1. Loading required passages (TensorFlow, NumPy, Pandas, CV2)\n",
    "\n",
    "Importing needed packages.\n",
    "\n",
    "**Essential Note:**\n",
    "If you would like to test _Keras Facenet Feature Extraction_ which I use here, you should use older versions of _Keras_, _TensorFlow_, and probably _Python_ as well. \n",
    "\n",
    "**My configuration of kernel for Keras Facenet:**\n",
    "- TensorFlow: 1.14.0\n",
    "- Keras: 2.3.1\n",
    "- Python: 3.6.2\n",
    "\n",
    "_P.S. That was quite painful to find this configuration and avoid all the conflicts :D_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1599dc83",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import pyplot from matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import TensorFlow image preprocessing \n",
    "from tensorflow.keras.preprocessing import image as image_preprocessing\n",
    "import tensorflow.keras.backend as tf_backend \n",
    "tf_backend.set_image_data_format('channels_last')\n",
    "\n",
    "# Importing fetch_lfw_people dataset from the sklearn.datasets package\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "\n",
    "# Import all other needed packages\n",
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import face_recognition\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "# Internal helper imports\n",
    "from helpers import graphics as help_graph\n",
    "from helpers import binary_converter as help_binary\n",
    "from helpers import accuracy_calculator as help_calc\n",
    "from helpers import shaper as help_shaper\n",
    "from helpers import vector_extractor as help_extract\n",
    "from helpers import segmenter as help_segmenter\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ff2372",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.2. Loading images dataset for checking the accuracy\n",
    "\n",
    "Note, that if you are trying to run this notebook, run only one block from either **Option 1** or **Option 2**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b37e343",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Option 1.** Importing `fetch_lfw_people` dataset from `sklearn.datasets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb86b42",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Importing dataset\n",
    "lfw_people_dataset = fetch_lfw_people(min_faces_per_person=80, color=True)\n",
    "\n",
    "# Settings images\n",
    "people_images = lfw_people_dataset.images\n",
    "# Setting labels\n",
    "people_labels = lfw_people_dataset.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541062c6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Option 2**. Importing `lfw` [dataset](http://vis-www.cs.umass.edu/lfw/) from the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63954f59",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Specifying path to the images\n",
    "lfw_dataset_path = './Images/lfw'\n",
    "# Getting array of subpaths\n",
    "lfw_people_subpaths = [subdirectory[0] for subdirectory in os.walk(lfw_dataset_path)]\n",
    "# Setting minimal amount of faces needed for one person\n",
    "faces_per_person = 60\n",
    "\n",
    "people_images, people_labels = [], []\n",
    "\n",
    "# Going along the array\n",
    "for person_id, subpath in enumerate(lfw_people_subpaths[1:]):\n",
    "    subpath_images = [cv2.cvtColor(cv2.imread(file), cv2.COLOR_BGR2RGB) for file in glob.glob(subpath + '/*.jpg')]\n",
    "    if len(subpath_images) < faces_per_person:\n",
    "        continue\n",
    "\n",
    "    for image in subpath_images[:faces_per_person]:\n",
    "        people_images.append(image)\n",
    "    \n",
    "    people_labels += [person_id] * faces_per_person\n",
    "    \n",
    "# Checking if loading was completed\n",
    "print(np.shape(people_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d24a5a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Converting `lfw_people` and `lfw_people_labels` to the np array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33058548",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "people_images_np = np.array(people_images, dtype = float)\n",
    "print('Shape of images dataset:', people_images_np.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29c377a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Displaying the dataset to check whether we initialized everything properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ae152f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gallery_offset = 0 # Can be changed to \"move\" the gallery\n",
    "gallery_width = 3 # Can be changed to resize gallery\n",
    "\n",
    "help_graph.display_images(\n",
    "    people_images_np[(gallery_offset):(gallery_width**2 + gallery_offset)], \n",
    "    people_labels[(gallery_offset):(gallery_width**2 + gallery_offset)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839f29e2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.3. Forming batches of images of the same person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc14e05c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "person_id = 0 # Person's ID to be displayed\n",
    "batch = help_shaper.form_batch_by_id(people_images_np, people_labels, image_id=person_id) # Getting images of a person\n",
    "\n",
    "print('Batch shape', batch.shape) # Check whether the shape is correct\n",
    "\n",
    "# Displaying a gallery\n",
    "help_graph.display_images(\n",
    "    batch[:(gallery_width**2)], \n",
    "    np.full((gallery_width**2), person_id, dtype=int), \n",
    "    gallery_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e811a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_id = 0 # Person's ID to be displayed\n",
    "batch = help_shaper.form_batch_by_id(people_images_np, people_labels, image_id=person_id) # Getting images of a person\n",
    "\n",
    "print('Batch shape', batch.shape) # Check whether the shape is correct\n",
    "\n",
    "# Displaying a gallery\n",
    "help_graph.display_images(\n",
    "    batch[:(gallery_width**2)], \n",
    "    np.full((gallery_width**2), person_id, dtype=int), \n",
    "    gallery_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871073d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributing images into batches of the same id\n",
    "image_batches = help_shaper.form_batches_list(people_images_np, people_labels)\n",
    "# Id of a batch to display in the gallery\n",
    "image_batch_display_id = 3\n",
    "\n",
    "# Trying to use form_batches_numpy\n",
    "image_batches_numpy = help_shaper.form_batches_numpy(people_images_np, people_labels, dataset='sklearn_lfw_people')\n",
    "\n",
    "# Displaying a gallery\n",
    "help_graph.display_images(\n",
    "    image_batches_numpy[image_batch_display_id][:(gallery_width**2)], \n",
    "    np.full((gallery_width**2), image_batch_display_id, dtype=int), \n",
    "    gallery_size=3)\n",
    "\n",
    "# Printing some info to verify that everything was implemented properly\n",
    "print('Number of batches:', len(image_batches))\n",
    "print('Shape of selected batch:', image_batches[image_batch_display_id].shape)\n",
    "print('Shape of numpy list of batches:', image_batches_numpy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd165e5c",
   "metadata": {},
   "source": [
    "# Section 2. Extracting features from an image using Keras Facenet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c2c77c",
   "metadata": {},
   "source": [
    "#### Loading model's weights and model itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8505d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('models/facenet_keras.h5', compile=False)model = keras.models.load_model('models/facenet_keras.h5', compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6c2d4e",
   "metadata": {},
   "source": [
    "Suppose we have a batch of size $n_b$ with images $I^{(1)}, I^{(2)}, \\dots, I^{(n_b)}$.\n",
    "Using FaceNet, we can extract a feature vector of dimension $n_f$, that is:\n",
    "$$\n",
    "f(I^{(j)}) = \\begin{pmatrix} f(I^{(j)})_1 \\\\ f(I^{(j)})_2 \\\\ \\vdots \\\\ f(I^{(j)})_{n_f} \\end{pmatrix}\n",
    "$$\n",
    "To evaluate difference $d(I^{(m)}, I^{(k)})$ between images $I^{(m)}$ and $I^{(k)}$ we are going to simply take a square of the feature-vectors difference norm:\n",
    "$$\n",
    "d(I^{(m)}, I^{(k)}) = \\| f(I^{(m)}) - f(I^{(k)}) \\|^2 = \\sum_{j=1}^{n_f} (f(I^{(m)})_j - f(I^{(k)})_j)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fbb8e3",
   "metadata": {},
   "source": [
    "### Section 2.1 (Optinal). Segmenting face from the picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b426eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmenting batches (OPTIONAL)\n",
    "image_batches_segmented = help_segmenter.segment_batches(image_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeca3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batches_segmented_np = help_shaper.normalize_batches_size(image_batches_segmented)\n",
    "print('Normalized batches array shape', np.shape(image_batches_segmented_np))\n",
    "\n",
    "help_graph.visualize_segmented(image_batches[4][3], image_batches_segmented_np[4][3], figure_size=7)\n",
    "help_graph.visualize_segmented(image_batches[1][10], image_batches_segmented_np[1][10], figure_size=7)\n",
    "help_graph.visualize_segmented(image_batches[2][20], image_batches_segmented_np[2][20], figure_size=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2abf47",
   "metadata": {},
   "source": [
    "**Note!** This line will overwrite the previously formed array `image_batches_numpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddacd6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating image_batches_numpy\n",
    "image_batches_numpy = image_batches_segmented_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dceee063",
   "metadata": {},
   "source": [
    "### Section 2.2. Retrieving the set of features for our images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa6b439",
   "metadata": {},
   "source": [
    "**Distance between vectors function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c30218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting array of feature vectors\n",
    "feature_vectors = help_extract.extract_vectors_from_batches(image_batches_numpy, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bc9b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "n1, i1, k1, j1 = 0, 0, 0, 6\n",
    "n2, i2, k2, j2 = 0, 0, 2, 15\n",
    "\n",
    "\n",
    "help_graph.distance_visualize(image_batches[n1][i1], \n",
    "                   image_batches[k1][j1], \n",
    "                   help_extract.feature_vectors_distance(feature_vectors[n1][i1], feature_vectors[k1][j1]))\n",
    "help_graph.distance_visualize(image_batches[n2][i2], \n",
    "                   image_batches[k2][j2],\n",
    "                   help_extract.feature_vectors_distance(feature_vectors[n2][i2], feature_vectors[k2][j2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357a0a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read images\n",
    "person_1 = np.array(Image.open('Images/Dima1.png'))\n",
    "person_2 = np.array(Image.open('Images/Dima2.png'))\n",
    "\n",
    "# Visualization of all distances\n",
    "help_graph.distance_visualize(person_1, person_2, help_extract.get_distance_between_images(person_1, person_2, model), 'distance')\n",
    "help_graph.distance_visualize(person_1, image_batches[2][3], help_extract.get_distance_between_images(person_1, image_batches[2][3], model), 'distance')\n",
    "help_graph.distance_visualize(person_1, image_batches[3][10], help_extract.get_distance_between_images(person_1, image_batches[3][10], model), 'distance')\n",
    "help_graph.distance_visualize(image_batches[1][15], \n",
    "                   image_batches[1][30], \n",
    "                   help_extract.get_distance_between_images(image_batches[1][15], image_batches[1][30], model), 'distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e300ab0",
   "metadata": {},
   "source": [
    "# Section 3. Estimating the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e5116c",
   "metadata": {},
   "source": [
    "### 3.1. Forming set of pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959edde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forming a list of pairs\n",
    "pairs = help_shaper.form_pairs_list((len(feature_vectors), help_shaper.get_minimal_batch_size(feature_vectors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310bb2b3",
   "metadata": {},
   "source": [
    "### 3.2. Finding the best threshold to estimate pair difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0454c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting arrays of threshold and accuracies\n",
    "thresholds, accuracies = help_calc.threshold_accuracies(pairs, feature_vectors, lower_edge=40.0, upper_edge=300.0, step=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876df9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "help_graph.draw_plot(thresholds, accuracies,\n",
    "                     X_label='Threshold', Y_label='Binary accuracy', \n",
    "                     title='Binary accuracy dependence on threshold', color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7756773",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_threshold = help_calc.get_best_threshold(pairs, feature_vectors, lower_edge=40.0, upper_edge=200.0, step=0.1)\n",
    "print('Best threshold is', best_threshold)\n",
    "print('Binary accuracy is', help_calc.calculate_pairs_binary_accuracy(pairs, feature_vectors, threshold=best_threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce78e8ab",
   "metadata": {},
   "source": [
    "### 3.2. Defining the term \"continious accuracy\" of pair difference prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f11db84",
   "metadata": {},
   "source": [
    "Now let us estimate roughly what is the accuracy of our algorithm.\n",
    "Suppose we have $n_b$ batches, every one of which contains $n_i$ images:\n",
    "$$\n",
    "(I^{(1)}_1, I^{(1)}_2, \\dots, I^{(1)}_{n_i}), (I^{(2)}_1, I^{(2)}_2, \\dots\\, I^{(2)}_{n_i}), \\dots, (I^{(n_b)}_1, \\dots, I^{(n_b)}_{n_i})\n",
    "$$\n",
    "In other words, $I^{(i)}_j$ is a $j$th image in $i$th batch.\n",
    "\n",
    "We could probably use binary classification loss functions or F1 score, but we are interested not in the face recognition task, but on the feature vectors themselves as they should not differ significantly.\n",
    "\n",
    "We will define the following \"accuracy\" function, where $\\tau$ denotes certain threshold (in our case we have put $\\tau = 100$):\n",
    "$$\n",
    "\\alpha(I^{(n_1)}_{i_1}, I^{(n_2)}_{i_2}) = \n",
    "\\begin{cases}\n",
    "\\max\\{1- \\left(\\frac{d(I^{(n)}_{i_1}, I^{(n)}_{i_2})}{\\tau}\\right)^{\\eta_1}, 0\\} \\; \\text{if} \\; n_1 = n_2 = n \\\\\n",
    "\\max\\{1-\\left(\\frac{\\tau}{d(I^{(n_1)}_{i_1}, I^{(n_2)}_{i_2})}\\right)^{\\eta_2}, 0\\} \\; \\text{if} \\; n_1 \\neq n_2\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The intuition behind this formula is rather simple: if both images are from the same person, then if the value is less than threshold, then the value interpolates on a segment $[0,1]$ (if distance is close to $0$, thus accuracy is close to $1$, if distance is close to $\\tau$, then the accuracy tends to $0$). Same intuition if images are different, but we use the inverse relation to the power of $\\eta$.\n",
    "\n",
    "Suppose we chose $n_p$ pairs of images, each element of which has parameters $p^{(t)} = \\{n^{(t)}_1, n^{(t)}_2, i^{(t)}_1, i^{(t)}_2\\}$. Then our accuracy equals:\n",
    "$$\n",
    "\\mathcal{A} = \\sqrt{\\frac{1}{n_p} \\sum_{t=1}^{n_p} \\alpha^2(I^{(n^{(t)}_1)}_{i^{(t)}_1}, I^{(n^{(t)}_2)}_{i^{(t)}_2})}\n",
    "$$\n",
    "\n",
    "We will select batches so that the number of image pairs from the same batch is the same as the number of pairs from the different batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c85dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking whether calculation of accuracy is good enough \n",
    "\n",
    "# Expected value: 1 - (100/200)^3 = 0.875 \n",
    "print('If batches are different and distance=200, accuracy equals', help_calc.calculate_accuracy(False, 200))\n",
    "# Expected value: 0 \n",
    "print('If batches are the same and distance=200, accuracy equals', help_calc.calculate_accuracy(True, 200))\n",
    "# Expected value: 0\n",
    "print('If batches are the different and distance=90, accuracy equals', help_calc.calculate_accuracy(False, 90))\n",
    "# Expected value: 1 - (64/100)^(3) = 0.737856 \n",
    "print('If batches are the same and distance=64, accuracy equals', help_calc.calculate_accuracy(True, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42e0b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization of accuracy between images\n",
    "\n",
    "help_graph.distance_visualize(image_batches[0][0], \n",
    "                   image_batches[0][1], \n",
    "                   help_calc.calculate_pair_accuracy(feature_vectors, 0, 0, 0, 1, threshold=best_threshold))\n",
    "help_graph.distance_visualize(image_batches[0][0], \n",
    "                   image_batches[1][0], \n",
    "                   help_calc.calculate_pair_accuracy(feature_vectors, 0, 0, 1, 0, threshold=best_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773f1e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = help_calc.calculate_pairs_accuracy([[0, 0, 0, 1], [0, 0, 1, 0]], feature_vectors, threshold=best_threshold)\n",
    "print('Cumulative accuracy for two images above is', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be206ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting accuracy\n",
    "print('Total accuracy is', help_calc.calculate_pairs_accuracy(pairs, feature_vectors, threshold=best_threshold))\n",
    "print('Total binary accuracy is', help_calc.calculate_pairs_binary_accuracy(pairs, feature_vectors, threshold=best_threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0433c6e8",
   "metadata": {},
   "source": [
    "# Saving the feature vector data to the CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f93e434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_row_csv(feature_vectors, person_id, image_id):\n",
    "    \"\"\"\n",
    "    Returns an array that corresponds to a single row in the csv file\n",
    "    \"\"\"\n",
    "    \n",
    "    row = [person_id, image_id]\n",
    "    return np.append(row, feature_vectors[person_id][image_id])\n",
    "\n",
    "def form_rows_csv(feature_vectors):\n",
    "    \"\"\"\n",
    "    Returns an array with rows used to load the csv file\n",
    "    \"\"\"\n",
    "    \n",
    "    rows = []\n",
    "    for person_id in range(len(feature_vectors)):\n",
    "        for image_id in range(len(feature_vectors[person_id])):\n",
    "            rows.append(form_row_for_csv(feature_vectors, person_id, image_id))\n",
    "            \n",
    "    return rows\n",
    "\n",
    "def form_header_csv(vector_dimension):\n",
    "    \"\"\"\n",
    "    Forms a header for the CSV file\n",
    "    \n",
    "    Input:\n",
    "    vector_dimension -- integer that represents the dimensionality of the vector\n",
    "    \"\"\"\n",
    "    \n",
    "    header = ['person_id', 'image_id']\n",
    "    for vector_id in range(vector_dimension):\n",
    "        header.append('feature_' + str(vector_id))\n",
    "        \n",
    "    return header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3de5a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Load the CSV file\n",
    "with open('datasets/keras_facenet_feature_vectors.csv', 'w') as dataset:\n",
    "    # Create a writer\n",
    "    writer = csv.writer(dataset)\n",
    "    \n",
    "    # Write a header\n",
    "    vector_dimension = len(feature_vectors[0][0])\n",
    "    writer.writerow(form_header_csv(vector_dimension))\n",
    "    \n",
    "    # Write rows\n",
    "    writer.writerows(form_rows_csv(feature_vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b7bd7f",
   "metadata": {},
   "source": [
    "# Section 4. Binary string formation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a95c2c6",
   "metadata": {},
   "source": [
    "Suppose we would like to form a binary string $s(I)=s_1s_2\\dots s_k$ from a feature vector $\\mathbf{f}(I) \\in \\mathbb{R}^{k}$ of some image $I$:\n",
    "$$\n",
    "\\mathbf{f}(I) = \\begin{bmatrix} f(I)_1 \\\\ f(I)_2 \\\\ \\dots \\\\ f(I)_k \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then, we will form the binary string according to the following rule:\n",
    "$$\n",
    "s_j = \\begin{cases} 1, f(I)_j > 0 \\\\ 0, f(I)_j \\leq 0 \\end{cases}\n",
    "$$\n",
    "\n",
    "Also we will denote the distance between two binary string $s=s_1s_2\\dots s_k$ and $h=h_1h_2 \\dots h_k$ as follows:\n",
    "$$\n",
    "\\delta(s, h) = \\frac{1}{k} \\sum_{j=1}^k |s_j - h_j|\n",
    "$$\n",
    "\n",
    "The similarity of two binary string we will denote as $\\sigma(s, h) = 1 - \\delta(s,h)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0224d1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forming the string batches from the feature vectors\n",
    "string_batches = help_binary.form_string_batches(feature_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b94a76",
   "metadata": {},
   "source": [
    "Using the set of pairs formed before, we can split it into the set of pairs with the same people and with different people (each of size $n_p' = n_p/2$). To get the average similarity between two same or different people, we will simple average the similarity function applied to each pair in the corresponding set, that is:\n",
    "$$\n",
    "\\sigma_{\\text{same/different}} = \\frac{1}{n_p'}\\sum_{t=1}^{n_p'} \\sigma(I_{i_1^{[t]}}^{(n_1^{[t]})}, I_{i_2^{[t]}}^{(n_2^{[t]})})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e412fc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "same_pairs, different_pairs = help_shaper.split_pairs(pairs)\n",
    "\n",
    "print('Average similarity between same person', help_binary.get_average_similarity(string_batches, same_pairs))\n",
    "print('Average similarity between different people', help_binary.get_average_similarity(string_batches, different_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7620836",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras-facenet",
   "language": "python",
   "name": "keras-facenet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
